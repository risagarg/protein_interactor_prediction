{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1b70ea6",
   "metadata": {},
   "source": [
    "# Feature Engineering Pipeline\n",
    "\n",
    "This notebook demonstrates the complete feature engineering pipeline for protein interactor prediction, including:\n",
    "\n",
    "1. **UniProt Data Extraction**: Subcellular locations, GO terms, protein domains, and PTMs\n",
    "2. **Ensembl Mapping**: Converting UniProt IDs to Ensembl gene IDs for HPA data\n",
    "3. **HPA Integration**: Brain tissue expression data from Human Protein Atlas (in this example, but you can change this)\n",
    "4. **ESM-2 Embeddings**: 1280-dimensional protein sequence embeddings\n",
    "5. **Multi-hot Encoding**: Converting categorical features to ML-ready format\n",
    "\n",
    "The pipeline processes protein IDs and generates comprehensive feature matrices suitable for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f1eec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import os\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab0b036",
   "metadata": {},
   "source": [
    "## 1. UniProt Data Extraction\n",
    "\n",
    "The `get_uniprot_data()` function extracts comprehensive protein annotations from UniProt, including:\n",
    "- **Subcellular locations** from comments, features, keywords, and GO terms\n",
    "- **GO terms** for functional annotations\n",
    "- **Protein domains** from InterPro cross-references\n",
    "- **Post-translational modifications (PTMs)** from features and keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8560ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_get(url, headers=None, max_retries=5, sleep_time=1):\n",
    "    \"\"\"Robust HTTP GET with retry logic for API calls.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            elif response.status_code == 429:\n",
    "                print(f\"Rate limited! Sleeping and retrying... (attempt {attempt+1})\")\n",
    "                time.sleep(5 * (attempt + 1))\n",
    "            else:\n",
    "                print(f\"Status {response.status_code} for {url}\")\n",
    "                time.sleep(sleep_time)\n",
    "        except Exception as e:\n",
    "            print(f\"Error on attempt {attempt+1} for {url}: {e}\")\n",
    "            time.sleep(sleep_time)\n",
    "    print(f\"Failed to get {url} after {max_retries} attempts.\")\n",
    "    return None\n",
    "\n",
    "def get_uniprot_data(protein_id):\n",
    "    \"\"\"Extract comprehensive protein annotations from UniProt.\"\"\"\n",
    "    accession = protein_id\n",
    "    if '_HUMAN' in protein_id:\n",
    "        url = f\"https://rest.uniprot.org/uniprotkb/search?query={protein_id}&fields=accession&format=json\"\n",
    "        r = robust_get(url)\n",
    "        if r:\n",
    "            data = r.json()\n",
    "            if data.get('results'):\n",
    "                accession = data['results'][0]['primaryAccession']\n",
    "            else:\n",
    "                return {'locations': [], 'go_terms': [], 'domains': [], 'ptms': []}\n",
    "        else:\n",
    "            return {'locations': [], 'go_terms': [], 'domains': [], 'ptms': []}\n",
    "    \n",
    "    base_url = f\"https://rest.uniprot.org/uniprotkb/{accession}.json\"\n",
    "    response = robust_get(base_url)\n",
    "    if response:\n",
    "        data = response.json()\n",
    "        \n",
    "        # --- Subcellular locations ---\n",
    "        locations = set()\n",
    "        # From comments\n",
    "        for comment in data.get(\"comments\", []):\n",
    "            if comment.get(\"commentType\") == \"SUBCELLULAR LOCATION\":\n",
    "                for loc in comment.get(\"subcellularLocations\", []):\n",
    "                    if \"location\" in loc:\n",
    "                        locations.add(loc[\"location\"].get(\"value\"))\n",
    "        # From features\n",
    "        for feature in data.get(\"features\", []):\n",
    "            if feature.get(\"type\", \"\").upper() == \"SUBCELLULAR LOCATION\":\n",
    "                desc = feature.get(\"description\")\n",
    "                if desc:\n",
    "                    locations.add(desc)\n",
    "        # From keywords\n",
    "        for kw in data.get(\"keywords\", []):\n",
    "            val = kw.get(\"value\", \"\").lower()\n",
    "            if (\n",
    "                \"subcellular location\" in val or\n",
    "                \"membrane\" in val or\n",
    "                \"cytoplasm\" in val or\n",
    "                \"nucleus\" in val or\n",
    "                \"mitochondrion\" in val\n",
    "            ):\n",
    "                locations.add(kw.get(\"value\"))\n",
    "        # From GO terms: add GO:CC (Cellular Component) terms to locations\n",
    "        for xref in data.get(\"uniProtKBCrossReferences\", []):\n",
    "            if xref.get(\"database\") == \"GO\":\n",
    "                properties = xref.get(\"properties\", [])\n",
    "                for prop in properties:\n",
    "                    if prop.get(\"term\") == \"C\":  # Cellular Component\n",
    "                        go_loc = prop.get(\"value\")\n",
    "                        if go_loc:\n",
    "                            locations.add(go_loc)\n",
    "        \n",
    "        # --- GO terms ---\n",
    "        go_terms = []\n",
    "        for xref in data.get(\"uniProtKBCrossReferences\", []):\n",
    "            if xref.get(\"database\") == \"GO\":\n",
    "                term = xref.get(\"properties\", [{}])[0].get(\"value\", \"\").lower()\n",
    "                go_terms.append(term)\n",
    "        \n",
    "        # --- Domains ---\n",
    "        domains = []\n",
    "        for xref in data.get(\"uniProtKBCrossReferences\", []):\n",
    "            if xref.get(\"database\") == \"InterPro\":\n",
    "                domains.append(xref.get(\"id\"))\n",
    "        \n",
    "        # --- PTMs (expanded logic) ---\n",
    "        ptms = set()\n",
    "        # From features: include any feature with PTM-related type or description\n",
    "        for feature in data.get(\"features\", []):\n",
    "            ftype = feature.get(\"type\", \"\").upper()\n",
    "            desc = feature.get(\"description\", \"\")\n",
    "            # If the type or description suggests a PTM, include it\n",
    "            if (\n",
    "                \"PTM\" in ftype or\n",
    "                \"MOD\" in ftype or\n",
    "                \"MOD_RES\" in ftype or\n",
    "                \"CARBOHYD\" in ftype or\n",
    "                \"LIPID\" in ftype or\n",
    "                \"GLYCOSYLATION\" in ftype or\n",
    "                \"PHOSPHO\" in ftype or\n",
    "                \"UBIQUITIN\" in ftype or\n",
    "                \"SUMO\" in ftype or\n",
    "                \"ACETYL\" in ftype or\n",
    "                \"METHYL\" in ftype or\n",
    "                \"DISULFID\" in ftype or\n",
    "                \"CROSSLNK\" in ftype or\n",
    "                \"GPI_ANCHOR\" in ftype or\n",
    "                \"PEPTIDE\" in ftype or\n",
    "                \"PROPEP\" in ftype or\n",
    "                \"SIGNAL\" in ftype or\n",
    "                \"TRANSIT\" in ftype or\n",
    "                \"CHAIN\" in ftype or\n",
    "                \"SE_CYS\" in ftype or\n",
    "                \"NON_STD\" in ftype or\n",
    "                \"VARIANT\" in ftype or\n",
    "                \"VAR_SEQ\" in ftype or\n",
    "                \"MUTAGEN\" in ftype or\n",
    "                \"TOPO_DOM\" in ftype or\n",
    "                \"ZN_FING\" in ftype or\n",
    "                \"COILED\" in ftype or\n",
    "                \"COMPBIAS\" in ftype or\n",
    "                \"REGION\" in ftype or\n",
    "                \"REPEAT\" in ftype or\n",
    "                \"MOTIF\" in ftype or\n",
    "                \"BINDING\" in ftype or\n",
    "                \"SITE\" in ftype or\n",
    "                \"METAL\" in ftype or\n",
    "                \"NP_BIND\" in ftype or\n",
    "                \"DNA_BIND\" in ftype or\n",
    "                \"CA_BIND\" in ftype or\n",
    "                # Also check description for PTM-related words\n",
    "                \"ptm\" in desc.lower() or\n",
    "                \"modification\" in desc.lower() or\n",
    "                \"glycosylation\" in desc.lower() or\n",
    "                \"phospho\" in desc.lower() or\n",
    "                \"ubiquitin\" in desc.lower() or\n",
    "                \"sumo\" in desc.lower() or\n",
    "                \"acetyl\" in desc.lower() or\n",
    "                \"methyl\" in desc.lower() or\n",
    "                \"disulfide\" in desc.lower() or\n",
    "                \"cross-link\" in desc.lower()\n",
    "            ):\n",
    "                if desc:\n",
    "                    ptms.add(desc)\n",
    "        # From keywords\n",
    "        for kw in data.get(\"keywords\", []):\n",
    "            val = kw.get(\"value\", \"\").lower()\n",
    "            if (\n",
    "                \"ptm\" in val or\n",
    "                \"modification\" in val or\n",
    "                \"glycosylation\" in val or\n",
    "                \"phospho\" in val or\n",
    "                \"ubiquitin\" in val or\n",
    "                \"sumoylation\" in val or\n",
    "                \"acetylation\" in val or\n",
    "                \"methylation\" in val or\n",
    "                \"disulfide\" in val or\n",
    "                \"cross-link\" in val\n",
    "            ):\n",
    "                ptms.add(kw.get(\"value\"))\n",
    "        \n",
    "        return {\n",
    "            'locations': list(locations),\n",
    "            'go_terms': list(set(go_terms)),\n",
    "            'domains': list(set(domains)),\n",
    "            'ptms': list(ptms)\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Failed to fetch UniProt data for {protein_id}\")\n",
    "        return {'locations': [], 'go_terms': [], 'domains': [], 'ptms': []}"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297b5224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_map_uniprot_to_ensembl(uniprot_id):\n",
    "    \"\"\"Maximally robust mapping: try all strategies with both original and cleaned IDs.\"\"\"\n",
    "    def clean_protein_id(protein_id):\n",
    "        protein_id = str(protein_id)\n",
    "        base_id = protein_id.split('-')[0]\n",
    "        base_id = base_id.replace('_HUMAN', '')\n",
    "        return base_id\n",
    "\n",
    "    # Helper for transcript-to-gene mapping\n",
    "    def get_ensembl_id_from_transcript(transcript_id):\n",
    "        url = f\"https://rest.ensembl.org/lookup/id/{transcript_id}?\"\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        response = robust_get(url, headers=headers)\n",
    "        if response:\n",
    "            data = response.json()\n",
    "            return data.get('Parent')\n",
    "        return None\n",
    "\n",
    "    # Helper for gene name lookup\n",
    "    def get_ensembl_id_from_gene_name(gene_name):\n",
    "        if not gene_name:\n",
    "            return None\n",
    "        url = f\"https://rest.ensembl.org/xrefs/symbol/homo_sapiens/{gene_name}?\"\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        response = robust_get(url, headers=headers)\n",
    "        if response:\n",
    "            results = response.json()\n",
    "            if results and len(results) > 0:\n",
    "                for result in results:\n",
    "                    gene_id = result.get('id')\n",
    "                    if gene_id and gene_id.startswith('ENSG'):\n",
    "                        return gene_id\n",
    "        return None\n",
    "\n",
    "    for test_id in [uniprot_id, clean_protein_id(uniprot_id)]:\n",
    "        print(f\"\\nProcessing {test_id}\")\n",
    "        uniprot_url = f\"https://rest.uniprot.org/uniprotkb/{test_id}.json\"\n",
    "        try:\n",
    "            response = requests.get(uniprot_url)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                # Method 1: Direct Ensembl cross-reference\n",
    "                for xref in data.get(\"uniProtKBCrossReferences\", []):\n",
    "                    if xref.get(\"database\") == \"Ensembl\":\n",
    "                        for prop in xref.get(\"properties\", []):\n",
    "                            if prop.get(\"key\") == \"GeneId\":\n",
    "                                gene_id = prop.get(\"value\")\n",
    "                                if gene_id and gene_id.startswith('ENSG'):\n",
    "                                    print(f\"Found Ensembl gene ID (xref): {gene_id}\")\n",
    "                                    return gene_id\n",
    "                # Method 2: Transcript-to-gene mapping\n",
    "                for xref in data.get(\"uniProtKBCrossReferences\", []):\n",
    "                    if xref.get(\"database\") == \"Ensembl\":\n",
    "                        transcript_id = xref.get(\"id\")\n",
    "                        if transcript_id and transcript_id.startswith(\"ENST\"):\n",
    "                            gene_id = get_ensembl_id_from_transcript(transcript_id)\n",
    "                            if gene_id and gene_id.startswith('ENSG'):\n",
    "                                print(f\"Found Ensembl gene ID (transcript): {gene_id}\")\n",
    "                                return gene_id\n",
    "                # Method 3: Gene name lookup\n",
    "                genes = data.get('genes', [])\n",
    "                if genes and genes[0].get('geneName'):\n",
    "                    gene_name = genes[0]['geneName'].get('value')\n",
    "                    print(f\"Found gene name: {gene_name}\")\n",
    "                    gene_id = get_ensembl_id_from_gene_name(gene_name)\n",
    "                    if gene_id and gene_id.startswith('ENSG'):\n",
    "                        print(f\"Found Ensembl gene ID (gene name): {gene_id}\")\n",
    "                        return gene_id\n",
    "        except Exception as e:\n",
    "            print(f\"Error mapping {test_id} to Ensembl: {str(e)}\")\n",
    "    print(f\"No Ensembl Gene ID found for {uniprot_id} after all mapping strategies\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4827731",
   "metadata": {},
   "source": [
    "## 3. Human Protein Atlas Integration\n",
    "\n",
    "The `get_hpa_brain_ntpm_from_xml()` function extracts brain tissue expression data from HPA XML files, including:\n",
    "- nTPM values for all brain regions (nTPM > 1)\n",
    "- Brain expression summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b596b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hpa_brain_ntpm_from_xml(ensembl_id, sleep_time=1):\n",
    "    \"\"\"Fetches all human brain region nTPM values (nTPM > 1) and the summary field from HPA XML.\"\"\"\n",
    "    core_ensembl_id = ensembl_id.split('.')[0]  # Always use the core Ensembl ID (no version) for HPA queries!\n",
    "    \n",
    "    url = f\"https://www.proteinatlas.org/{core_ensembl_id}.xml\"\n",
    "    r = requests.get(url)\n",
    "    if r.status_code != 200:\n",
    "        print(f\"Failed to fetch XML for {ensembl_id}\")\n",
    "        return {}, None\n",
    "    \n",
    "    soup = BeautifulSoup(r.content, \"xml\")\n",
    "    ntpm_dict = {}\n",
    "    summary = None\n",
    "    \n",
    "    # Find the <rnaExpression> block for human brain\n",
    "    for rna_expr in soup.find_all(\"rnaExpression\"):\n",
    "        if rna_expr.get(\"assayType\") == \"humanBrain\":\n",
    "            # Get all <data> blocks (one per region)\n",
    "            for data in rna_expr.find_all(\"data\"):\n",
    "                tissue_tag = data.find(\"tissue\")\n",
    "                if tissue_tag:\n",
    "                    region = tissue_tag.text.strip()\n",
    "                    # Find the nTPM value\n",
    "                    for level in data.find_all(\"level\"):\n",
    "                        if level.get(\"type\") == \"normalizedRNAExpression\" and level.get(\"unitRNA\") == \"nTPM\":\n",
    "                            try:\n",
    "                                ntpm_val = float(level.get(\"expRNA\"))\n",
    "                                if ntpm_val > 1:\n",
    "                                    ntpm_dict[region] = ntpm_val\n",
    "                            except Exception:\n",
    "                                continue\n",
    "            # Get the summary field if present\n",
    "            summary_tag = rna_expr.find(\"rnaDistribution\")\n",
    "            if summary_tag:\n",
    "                summary = summary_tag.text.strip()\n",
    "            break  # Only one humanBrain block\n",
    "    \n",
    "    time.sleep(sleep_time)\n",
    "    return ntpm_dict, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69234e03",
   "metadata": {},
   "source": [
    "## 4. ESM-2 Protein Language Model\n",
    "\n",
    "The ESM-2 model generates 1280-dimensional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5b7c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ESM2 model and tokenizer\n",
    "model_name = \"facebook/esm2_t33_650M_UR50D\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "def get_sequence_embedding(sequence, tokenizer, model):\n",
    "    \"\"\"Generate ESM-2 embedding for a protein sequence.\"\"\"\n",
    "    # Tokenize sequence\n",
    "    inputs = tokenizer(sequence, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get mean of last hidden states (excluding padding tokens)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    # Calculate mean embedding (excluding padding tokens)\n",
    "    mean_embedding = (last_hidden_states * attention_mask.unsqueeze(-1)).sum(dim=1) / attention_mask.sum(dim=1, keepdim=True)\n",
    "    return mean_embedding.cpu().numpy()[0]\n",
    "\n",
    "print(f\"ESM-2 model loaded on {device}\")\n",
    "print(f\"Embedding dimension: 1280\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28dca29",
   "metadata": {},
   "source": [
    "## 5. Multi-hot Encoding Pipeline\n",
    "\n",
    "Conversion into binary ML features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f3659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_hot(df, col):\n",
    "    \"\"\"Convert list columns to multi-hot encoded binary features.\"\"\"\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    values = df[col].apply(lambda x: x if isinstance(x, list) else [])\n",
    "    expanded = pd.DataFrame(mlb.fit_transform(values), columns=[f\"{col}_{c}\" for c in mlb.classes_], index=df.index)\n",
    "    return expanded\n",
    "\n",
    "def process_features_to_ml_format(features_df):\n",
    "    \"\"\"Convert JSON features to ML-ready format with multi-hot encoding.\"\"\"\n",
    "    print(\"Converting to DataFrame...\")\n",
    "    features_df = pd.DataFrame.from_dict(features_df, orient='index')\n",
    "    features_df.index.name = 'protein_id'\n",
    "    features_df = features_df.reset_index()\n",
    "    print(\"Created features_df:\", features_df.shape)\n",
    "\n",
    "    print(\"Flattening uniprot_data and hpa_data...\")\n",
    "    uniprot_df = pd.json_normalize(features_df['uniprot_data'])\n",
    "    uniprot_df.columns = [f'uniprot_{col}' for col in uniprot_df.columns]\n",
    "    hpa_df = pd.json_normalize(features_df['hpa_data'])\n",
    "    hpa_df.columns = [f'hpa_{col}' for col in hpa_df.columns]\n",
    "    flat_df = pd.concat([features_df[['protein_id', 'ensembl_id', 'status']], uniprot_df, hpa_df], axis=1)\n",
    "    print(\"Flattened DataFrame:\", flat_df.shape)\n",
    "\n",
    "    # --- Multi-hot encode list features ---\n",
    "    multi_hot_cols = ['uniprot_locations', 'uniprot_go_terms', 'uniprot_domains', 'uniprot_ptms']\n",
    "    multi_hot_dfs = []\n",
    "    for col in multi_hot_cols:\n",
    "        if col in flat_df.columns:\n",
    "            print(f\"Multi-hot encoding {col}...\")\n",
    "            mh = multi_hot(flat_df, col)\n",
    "            print(f\"  {col}: {mh.shape[1]} columns\")\n",
    "            multi_hot_dfs.append(mh)\n",
    "            flat_df = flat_df.drop(columns=[col])\n",
    "\n",
    "    if multi_hot_dfs:\n",
    "        flat_df = pd.concat([flat_df] + multi_hot_dfs, axis=1)\n",
    "    print(\"After multi-hot encoding:\", flat_df.shape)\n",
    "\n",
    "    # --- Rename nTPM columns to match training data format ---\n",
    "    print(\"Renaming nTPM columns...\")\n",
    "    ntpm_columns = {}\n",
    "    for col in flat_df.columns:\n",
    "        if col.startswith('hpa_all_tissue_expression.'):\n",
    "            # Extract region name\n",
    "            region = col.split('.', 1)[1]\n",
    "            # Convert to training data format (spaces, parentheses, etc.)\n",
    "            region = region.replace('_', ' ').replace('  ', ' ')  # Handle double spaces\n",
    "            # Add log2_nTPM prefix\n",
    "            new_col = f'log2_nTPM_{region}'\n",
    "            ntpm_columns[col] = new_col\n",
    "\n",
    "    flat_df = flat_df.rename(columns=ntpm_columns)\n",
    "    print(\"After renaming nTPM columns:\", flat_df.shape)\n",
    "\n",
    "    # --- Apply log2 transformation to nTPM values ---\n",
    "    print(\"Applying log2 transformation to nTPM values...\")\n",
    "    log2_ntpm_cols = [col for col in flat_df.columns if col.startswith('log2_nTPM_')]\n",
    "    for col in log2_ntpm_cols:\n",
    "        # Apply log2 transformation, handle NaN and zeros\n",
    "        flat_df[col] = flat_df[col].apply(lambda x: np.log2(x) if pd.notna(x) and x > 0 else 0)\n",
    "\n",
    "    print(\"After log2 transformation:\", flat_df.shape)\n",
    "\n",
    "    # --- Binary encode brain_expressed robustly ---\n",
    "    if 'hpa_brain_expressed' in flat_df.columns:\n",
    "        flat_df['hpa_brain_expressed'] = flat_df['hpa_brain_expressed'].fillna(False).astype(int)\n",
    "        print(\"Binary encoded hpa_brain_expressed.\")\n",
    "\n",
    "    print(\"Final DataFrame shape:\", flat_df.shape)\n",
    "    print(\"Memory usage (MB):\", flat_df.memory_usage(deep=True).sum() / 1e6)\n",
    "    \n",
    "    return flat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c889d7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_feature_engineering_pipeline(protein_ids, batch_size=1000, checkpoint_interval=100):\n",
    "    \"\"\"Complete feature engineering pipeline with batch processing and checkpoints.\"\"\"\n",
    "    print(f\"Starting feature engineering for {len(protein_ids)} proteins...\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame({'protein_id': list(set(protein_ids))})\n",
    "    print(f\"Loaded DataFrame with {len(df)} proteins\")\n",
    "    \n",
    "    results = {}\n",
    "    num_proteins = len(df)\n",
    "    \n",
    "    for batch_start in range(0, num_proteins, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, num_proteins)\n",
    "        batch_df = df.iloc[batch_start:batch_end]\n",
    "        print(f\"\\nProcessing batch {batch_start} to {batch_end-1}...\")\n",
    "        \n",
    "        # Define checkpoint path\n",
    "        batch_checkpoint_path = f\"checkpoint_{batch_start}_{batch_end-1}.json\"\n",
    "        if os.path.exists(batch_checkpoint_path):\n",
    "            print(f\"Checkpoint for batch {batch_start}-{batch_end-1} exists, loading...\")\n",
    "            with open(batch_checkpoint_path, 'r') as f:\n",
    "                batch_results = json.load(f)\n",
    "        else:\n",
    "            batch_results = {}\n",
    "        \n",
    "        checkpoint_counter = 0\n",
    "        \n",
    "        try:\n",
    "            for i, protein_id in enumerate(tqdm(batch_df['protein_id'])):\n",
    "                if str(protein_id) in batch_results:\n",
    "                    continue  # Skip already processed\n",
    "                \n",
    "                print(f\"Processing {batch_start + i}: {protein_id}\")\n",
    "                try:\n",
    "                    # Get UniProt data\n",
    "                    uniprot_data = get_uniprot_data(protein_id)\n",
    "                    \n",
    "                    # Get Ensembl mapping\n",
    "                    ensembl_id = manual_map_uniprot_to_ensembl(protein_id)\n",
    "                    \n",
    "                    # Get HPA data\n",
    "                    ntpm_dict, brain_summary = {}, None\n",
    "                    if ensembl_id:\n",
    "                        try:\n",
    "                            ntpm_dict, brain_summary = get_hpa_brain_ntpm_from_xml(ensembl_id)\n",
    "                        except Exception as hpa_e:\n",
    "                            print(f\"Error extracting HPA XML for {protein_id}: {hpa_e}\")\n",
    "                    \n",
    "                    # Determine brain expression\n",
    "                    brain_expressed = (len(ntpm_dict) > 0) or (\n",
    "                        brain_summary and (\n",
    "                            brain_summary.lower().startswith(\"detected in all\") or\n",
    "                            brain_summary.lower().startswith(\"detected in many\")\n",
    "                        )\n",
    "                    )\n",
    "                    \n",
    "                    hpa_data = {\n",
    "                        'brain_expressed': brain_expressed,\n",
    "                        'all_tissue_expression': ntpm_dict,\n",
    "                        'brain_expression_summary': brain_summary\n",
    "                    }\n",
    "                    \n",
    "                    if ensembl_id:\n",
    "                        status = 'mapped'\n",
    "                    else:\n",
    "                        status = 'no_ensembl_mapping'\n",
    "                    \n",
    "                    batch_results[str(protein_id)] = {\n",
    "                        'ensembl_id': ensembl_id,\n",
    "                        'hpa_data': hpa_data,\n",
    "                        'uniprot_data': uniprot_data,\n",
    "                        'status': status\n",
    "                    }\n",
    "                    \n",
    "                except Exception as inner_e:\n",
    "                    print(f\"Error processing {protein_id}: {inner_e}\")\n",
    "                    batch_results[str(protein_id)] = {\n",
    "                        'ensembl_id': None,\n",
    "                        'hpa_data': None,\n",
    "                        'uniprot_data': None,\n",
    "                        'status': f'error: {inner_e}'\n",
    "                    }\n",
    "                \n",
    "                checkpoint_counter += 1\n",
    "                if checkpoint_counter % checkpoint_interval == 0:\n",
    "                    print(f\"\\nSaving checkpoint for batch {batch_start}-{batch_end-1} after {checkpoint_counter} proteins...\")\n",
    "                    with open(batch_checkpoint_path, 'w') as f:\n",
    "                        json.dump(batch_results, f, indent=2)\n",
    "                    print(\"Checkpoint saved!\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError encountered in batch {batch_start}-{batch_end-1}: {str(e)}\")\n",
    "            print(\"Saving progress before exit...\")\n",
    "            with open(batch_checkpoint_path, 'w') as f:\n",
    "                json.dump(batch_results, f, indent=2)\n",
    "            print(\"Progress saved!\")\n",
    "            raise e\n",
    "        \n",
    "        # Save batch results\n",
    "        with open(batch_checkpoint_path, 'w') as f:\n",
    "            json.dump(batch_results, f, indent=2)\n",
    "        print(f\"Batch {batch_start}-{batch_end-1} complete and saved.\")\n",
    "        \n",
    "        # Merge into overall results\n",
    "        results.update(batch_results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage (commented out to avoid running)\n",
    "# protein_ids = ['P12345', 'Q67890', 'P11111']  # Example protein IDs\n",
    "# results = complete_feature_engineering_pipeline(protein_ids)\n",
    "# ml_ready_df = process_features_to_ml_format(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b02c4b8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "\n",
    "The pipeline can process thousands of proteins efficiently and generates features suitable for machine learning models."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
