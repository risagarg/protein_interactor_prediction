{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bacab39",
   "metadata": {},
   "source": [
    "# Model Development: Ensemble Learning Pipeline\n",
    "\n",
    "This notebook demonstrates the development of a sophisticated ensemble learning system for protein interactor prediction, featuring:\n",
    "\n",
    "1. **DBSCAN Clustering**: Automatic identification of core and specialist protein groups\n",
    "2. **Enhanced Feature Selection**: Model-based and univariate feature selection with class balancing\n",
    "3. **Three-Model Ensemble**: Core, Specialist, and Expansion models with optimized weights\n",
    "4. **Data Leakage Prevention**: Rigorous methodology ensuring no information leakage\n",
    "5. **Cross-Validation**: Robust validation with proper feature alignment\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a4555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold, SelectFromModel\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.optimize import minimize\n",
    "import pickle\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d7f8cb",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "\n",
    "Load training and holdout datasets with proper separation to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8f53d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load data\n",
    "print(\"1. Loading data...\")\n",
    "# Note: In actual implementation, these would be loaded from your data files\n",
    "# train_df = pd.read_pickle('path/to/train_set.pkl')\n",
    "# holdout_df = pd.read_pickle('path/to/holdout_set.pkl')\n",
    "\n",
    "# For demonstration, we'll show the expected data structure\n",
    "print(\"Expected data structure:\")\n",
    "print(\"- Training data: Contains features, labels, and protein IDs\")\n",
    "print(\"- Holdout data: Contains same features for final evaluation\")\n",
    "print(\"- Both datasets include ESM-2 embeddings and UniProt features\")\n",
    "\n",
    "# Example data loading (commented out for demonstration)\n",
    "# train_df = pd.read_pickle('/path/to/train_set.pkl')\n",
    "# holdout_df = pd.read_pickle('/path/to/holdout_set.pkl')\n",
    "# print(f\"Training data shape: {train_df.shape}\")\n",
    "# print(f\"Holdout data shape: {holdout_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e715a8e5",
   "metadata": {},
   "source": [
    "## 2. Enhanced Data Preprocessing\n",
    "\n",
    "Remove constant features and handle missing values to prepare clean data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de004ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_constant_features(X_data):\n",
    "    \"\"\"Remove constant features before selection\"\"\"\n",
    "    print(f\"  Original features: {X_data.shape[1]}\")\n",
    "    \n",
    "    # Remove constant features\n",
    "    selector = VarianceThreshold(threshold=0.0)\n",
    "    X_cleaned = selector.fit_transform(X_data)\n",
    "    selected_features = X_data.columns[selector.get_support()].tolist()\n",
    "    \n",
    "    print(f\"  Features after removing constants: {len(selected_features)}\")\n",
    "    return X_cleaned, selected_features\n",
    "\n",
    "def clean_data_for_model(X_data, y_data):\n",
    "    \"\"\"Clean data by removing constants and handling NaNs\"\"\"\n",
    "    # Get non-embedding features\n",
    "    feature_cols = [col for col in X_data.columns if col not in ['label', 'protein_id', 'embedding']]\n",
    "    X_clean = X_data[feature_cols].copy()\n",
    "    \n",
    "    # Remove constant features\n",
    "    X_cleaned, selected_features = remove_constant_features(X_clean)\n",
    "    \n",
    "    # Handle NaNs\n",
    "    X_cleaned = pd.DataFrame(X_cleaned, columns=selected_features)\n",
    "    X_cleaned = X_cleaned.fillna(0)\n",
    "    \n",
    "    return X_cleaned, selected_features\n",
    "\n",
    "print(\"Data preprocessing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915218ee",
   "metadata": {},
   "source": [
    "## 3. DBSCAN Clustering for Protein Grouping\n",
    "\n",
    "Use DBSCAN clustering to automatically identify core and specialist protein groups based on feature similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298a907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dbscan_clustering(train_df, orig_pos_df):\n",
    "    \"\"\"Run DBSCAN clustering to identify core and specialist proteins\"\"\"\n",
    "    print(\"3. Running DBSCAN clustering...\")\n",
    "    \n",
    "    # Load original positives to get their IDs\n",
    "    orig_pos_ids = set(orig_pos_df[orig_pos_df['label'] == 1].index)\n",
    "    \n",
    "    # Extract original positives from training data\n",
    "    train_orig_positives = train_df[train_df['protein_id'].isin(orig_pos_ids)].copy()\n",
    "    print(f\"Original positives in training: {len(train_orig_positives)}\")\n",
    "    \n",
    "    # Get numeric features for DBSCAN\n",
    "    orig_pos_features = orig_pos_df.drop('label', axis=1).columns\n",
    "    train_orig_positives = train_orig_positives[list(orig_pos_features)]\n",
    "    numeric_positives_df = train_orig_positives.select_dtypes(include=[np.number]).copy()\n",
    "    numeric_positives_df = numeric_positives_df.dropna(axis=0, how='any').copy()\n",
    "    \n",
    "    print(f\"Shape after dropping NaNs: {numeric_positives_df.shape}\")\n",
    "    \n",
    "    # Run DBSCAN\n",
    "    print(\"Running DBSCAN with eps=150, min_samples=10...\")\n",
    "    scaler = StandardScaler()\n",
    "    positives_scaled = scaler.fit_transform(numeric_positives_df)\n",
    "    pca = PCA(n_components=0.95, random_state=42)\n",
    "    positives_pca = pca.fit_transform(positives_scaled)\n",
    "    \n",
    "    dbscan = DBSCAN(eps=150, min_samples=10)\n",
    "    cluster_labels = dbscan.fit_predict(positives_pca)\n",
    "    \n",
    "    # Add cluster labels back\n",
    "    numeric_positives_df['cluster'] = cluster_labels\n",
    "    \n",
    "    # Analyze results\n",
    "    cluster_counts = numeric_positives_df['cluster'].value_counts()\n",
    "    print(\"Cluster sizes:\")\n",
    "    print(cluster_counts)\n",
    "    \n",
    "    # Get core and specialist indices\n",
    "    core_indices = numeric_positives_df[numeric_positives_df['cluster'] == 0].index\n",
    "    specialist_indices = numeric_positives_df[numeric_positives_df['cluster'] == -1].index\n",
    "    \n",
    "    print(f\"Core proteins: {len(core_indices)}\")\n",
    "    print(f\"Specialist proteins: {len(specialist_indices)}\")\n",
    "    \n",
    "    return core_indices, specialist_indices\n",
    "\n",
    "print(\"DBSCAN clustering function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb22ee6",
   "metadata": {},
   "source": [
    "## 4. Training Data Grouping\n",
    "\n",
    "Split training data into three groups: Core, Specialist, and Expansion models, each with their own positive and negative samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e2c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_training_data(train_df, core_indices, specialist_indices, lowconf_df):\n",
    "    \"\"\"Split training data into three groups for ensemble models\"\"\"\n",
    "    print(\"4. Splitting training data into groups...\")\n",
    "    \n",
    "    # Get core and specialist proteins from training data\n",
    "    core_positives = train_df[train_df['protein_id'].isin(core_indices)].copy()\n",
    "    specialist_positives = train_df[train_df['protein_id'].isin(specialist_indices)].copy()\n",
    "    \n",
    "    # Get low-confidence proteins from training data\n",
    "    lowconf_ids = set(lowconf_df.index)\n",
    "    expansion_positives = train_df[train_df['protein_id'].isin(lowconf_ids)].copy()\n",
    "    \n",
    "    # Get negatives\n",
    "    negatives = train_df[train_df['label'] == 0].copy()\n",
    "    \n",
    "    # Add negatives to each group\n",
    "    core_train = pd.concat([core_positives, negatives], axis=0)\n",
    "    specialist_train = pd.concat([specialist_positives, negatives], axis=0)\n",
    "    expansion_train = pd.concat([expansion_positives, negatives], axis=0)\n",
    "    \n",
    "    print(f\"Core training data: {len(core_train)} (positives: {len(core_positives)}, negatives: {len(negatives)})\")\n",
    "    print(f\"Specialist training data: {len(specialist_train)} (positives: {len(specialist_positives)}, negatives: {len(negatives)})\")\n",
    "    print(f\"Expansion training data: {len(expansion_train)} (positives: {len(expansion_positives)}, negatives: {len(negatives)})\")\n",
    "    \n",
    "    return core_train, specialist_train, expansion_train\n",
    "\n",
    "print(\"Training data splitting function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8078f8e",
   "metadata": {},
   "source": [
    "## 5. Enhanced Feature Selection\n",
    "\n",
    "Use both model-based and univariate feature selection with class balancing to find optimal features for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56798ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_features_enhanced(X_data, y_data, model_name):\n",
    "    \"\"\"Find optimal number of features with enhanced methods\"\"\"\n",
    "    print(f\"\\nFinding optimal features for {model_name}...\")\n",
    "    \n",
    "    # Clean data\n",
    "    X_clean, selected_features = clean_data_for_model(X_data, y_data)\n",
    "    \n",
    "    # Calculate class weights for imbalanced data\n",
    "    pos_weight = len(y_data[y_data == 0]) / len(y_data[y_data == 1])\n",
    "    print(f\"  Class weight (positives): {pos_weight:.2f}\")\n",
    "    \n",
    "    # Test different feature numbers\n",
    "    if model_name == \"Expansion\":\n",
    "        test_numbers = [50, 100, 200, 500, 1000, 2000, 3000]\n",
    "    else:\n",
    "        test_numbers = [50, 100, 200, 500, 1000]\n",
    "    \n",
    "    cv_scores = []\n",
    "    \n",
    "    for k in test_numbers:\n",
    "        if k <= len(selected_features):\n",
    "            print(f\"  Testing {k} features...\")\n",
    "            \n",
    "            # Use model-based selection instead of univariate\n",
    "            try:\n",
    "                selector = SelectFromModel(\n",
    "                    RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "                    max_features=k\n",
    "                )\n",
    "                X_k = selector.fit_transform(X_clean, y_data)\n",
    "                \n",
    "                # Fallback to univariate if model-based fails\n",
    "                if X_k.shape[1] == 0:\n",
    "                    selector = SelectKBest(score_func=f_classif, k=k)\n",
    "                    X_k = selector.fit_transform(X_clean, y_data)\n",
    "            except:\n",
    "                # Fallback to univariate selection\n",
    "                selector = SelectKBest(score_func=f_classif, k=k)\n",
    "                X_k = selector.fit_transform(X_clean, y_data)\n",
    "            \n",
    "            # Train with class weights\n",
    "            xgb_model = xgb.XGBClassifier(\n",
    "                objective='binary:logistic',\n",
    "                eval_metric='logloss',\n",
    "                n_estimators=100,\n",
    "                random_state=42,\n",
    "                scale_pos_weight=pos_weight\n",
    "            )\n",
    "            \n",
    "            cv_score = cross_val_score(xgb_model, X_k, y_data, cv=3, scoring='f1').mean()\n",
    "            cv_scores.append(cv_score)\n",
    "            \n",
    "            print(f\"    CV F1 score: {cv_score:.3f}\")\n",
    "    \n",
    "    # Find optimal number\n",
    "    if cv_scores:\n",
    "        optimal_k = test_numbers[np.argmax(cv_scores)]\n",
    "        best_score = max(cv_scores)\n",
    "        print(f\"  Optimal features for {model_name}: {optimal_k} (F1 = {best_score:.3f})\")\n",
    "        return optimal_k, best_score\n",
    "    else:\n",
    "        print(f\"  No valid features found for {model_name}\")\n",
    "        return 100, 0.0\n",
    "\n",
    "def get_optimal_features_enhanced(X_data, optimal_k, model_name):\n",
    "    \"\"\"Get optimal features with enhanced selection\"\"\"\n",
    "    # Clean data\n",
    "    X_clean, selected_features = clean_data_for_model(X_data, X_data['label'])\n",
    "    \n",
    "    # Try model-based selection first\n",
    "    try:\n",
    "        selector = SelectFromModel(\n",
    "            RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "            max_features=optimal_k\n",
    "        )\n",
    "        X_selected = selector.fit_transform(X_clean, X_data['label'])\n",
    "        selected_features = X_clean.columns[selector.get_support()].tolist()\n",
    "    except:\n",
    "        # Fallback to univariate selection\n",
    "        selector = SelectKBest(score_func=f_classif, k=optimal_k)\n",
    "        X_selected = selector.fit_transform(X_clean, X_data['label'])\n",
    "        selected_features = X_clean.columns[selector.get_support()].tolist()\n",
    "    \n",
    "    # Add embedding column\n",
    "    selected_features.append('embedding')\n",
    "    \n",
    "    print(f\"  {model_name} model: {len(selected_features)} features\")\n",
    "    return selected_features\n",
    "\n",
    "print(\"Feature selection functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae62d3fa",
   "metadata": {},
   "source": [
    "## 6. Data Leakage Prevention\n",
    "\n",
    "**Critical Section**: Explicit data leakage checks to ensure no holdout data is used in training or feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198eaf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_leakage(holdout_df, core_train, specialist_train, expansion_train):\n",
    "    \"\"\"Comprehensive data leakage check\"\"\"\n",
    "    print(\"\\n=== DATA LEAKAGE CHECK ===\")\n",
    "    \n",
    "    # Check if any holdout proteins are in training data\n",
    "    holdout_ids = set(holdout_df['protein_id'])\n",
    "    core_train_ids = set(core_train['protein_id'])\n",
    "    specialist_train_ids = set(specialist_train['protein_id'])\n",
    "    expansion_train_ids = set(expansion_train['protein_id'])\n",
    "    \n",
    "    core_overlap = holdout_ids & core_train_ids\n",
    "    specialist_overlap = holdout_ids & specialist_train_ids\n",
    "    expansion_overlap = holdout_ids & expansion_train_ids\n",
    "    \n",
    "    print(f\"Holdout proteins in core training: {len(core_overlap)}\")\n",
    "    print(f\"Holdout proteins in specialist training: {len(specialist_overlap)}\")\n",
    "    print(f\"Holdout proteins in expansion training: {len(expansion_overlap)}\")\n",
    "    \n",
    "    if len(core_overlap) == 0 and len(specialist_overlap) == 0 and len(expansion_overlap) == 0:\n",
    "        print(\"✅ NO DATA LEAKAGE DETECTED\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"⚠️  POTENTIAL DATA LEAKAGE DETECTED\")\n",
    "        return False\n",
    "\n",
    "print(\"Data leakage check function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d1a3b0",
   "metadata": {},
   "source": [
    "## 7. Model Training with Class Balancing\n",
    "\n",
    "Train XGBoost models with class weights to handle imbalanced datasets effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f00477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_features_enhanced(X_data, features, model_name):\n",
    "    \"\"\"Train a model with enhanced feature handling and class weights\"\"\"\n",
    "    # Get non-embedding features\n",
    "    non_embedding_features = [f for f in features if f != 'embedding']\n",
    "    X_non_embedding = X_data[non_embedding_features].fillna(0)\n",
    "    \n",
    "    # Convert embedding arrays to features directly\n",
    "    embedding_arrays = X_data['embedding'].tolist()\n",
    "    embedding_features = np.array(embedding_arrays)\n",
    "    \n",
    "    # Combine features\n",
    "    X_model = np.hstack([X_non_embedding.values, embedding_features])\n",
    "    y_model = X_data['label']\n",
    "    \n",
    "    # Calculate class weights\n",
    "    pos_weight = len(y_model[y_model == 0]) / len(y_model[y_model == 1])\n",
    "    \n",
    "    model = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        n_estimators=200,\n",
    "        random_state=42,\n",
    "        scale_pos_weight=pos_weight,\n",
    "        max_depth=6,  # Prevent overfitting\n",
    "        learning_rate=0.1\n",
    "    )\n",
    "    \n",
    "    model.fit(X_model, y_model)\n",
    "    \n",
    "    # Create feature names for the combined features\n",
    "    feature_names = list(X_non_embedding.columns) + [f'emb_{i}' for i in range(embedding_features.shape[1])]\n",
    "    \n",
    "    print(f\"  {model_name} model trained with {X_model.shape[1]} features (class weight: {pos_weight:.2f})\")\n",
    "    return model, feature_names\n",
    "\n",
    "print(\"Model training function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ff1c34",
   "metadata": {},
   "source": [
    "## 8. Ensemble Methods and Weight Optimization\n",
    "\n",
    "Implement multiple ensemble strategies and optimize weights for maximum performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e45afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_holdout_features_enhanced(holdout_df, final_features, model_name):\n",
    "    \"\"\"Prepare holdout data with enhanced feature handling\"\"\"\n",
    "    # Separate embedding and non-embedding features\n",
    "    non_embedding_features = [f for f in final_features if not f.startswith('emb_')]\n",
    "    \n",
    "    # Get non-embedding features that exist in holdout data\n",
    "    available_non_embedding = [f for f in non_embedding_features if f in holdout_df.columns]\n",
    "    X_non_embedding = holdout_df[available_non_embedding].fillna(0)\n",
    "    \n",
    "    # Convert embedding arrays to features directly\n",
    "    embedding_arrays = holdout_df['embedding'].tolist()\n",
    "    embedding_features = np.array(embedding_arrays)\n",
    "    \n",
    "    # Combine features\n",
    "    X_holdout = np.hstack([X_non_embedding.values, embedding_features])\n",
    "    \n",
    "    return X_holdout\n",
    "\n",
    "def optimize_weights_improved(core_probs, specialist_probs, expansion_probs, y_true):\n",
    "    \"\"\"Find optimal weights without sum constraint\"\"\"\n",
    "    def objective(weights):\n",
    "        ensemble_probs = (weights[0] * core_probs +\n",
    "                         weights[1] * specialist_probs +\n",
    "                         weights[2] * expansion_probs)\n",
    "        ensemble_preds = (ensemble_probs >= 0.5).astype(int)\n",
    "        return -f1_score(y_true, ensemble_preds)  # Negative because we minimize\n",
    "    \n",
    "    # Remove sum constraint, allow any positive weights\n",
    "    bounds = [(0, 3), (0, 3), (0, 3)]\n",
    "    \n",
    "    result = minimize(objective, [1.0, 1.0, 1.0], bounds=bounds, method='L-BFGS-B')\n",
    "    \n",
    "    if result.success:\n",
    "        optimal_weights = result.x\n",
    "        optimal_f1 = -result.fun\n",
    "        return optimal_weights, optimal_f1\n",
    "    else:\n",
    "        print(\"    Weight optimization failed, using default weights\")\n",
    "        return [1.0, 1.0, 1.0], f1_score(y_true, (core_probs >= 0.5).astype(int))\n",
    "\n",
    "def evaluate_ensemble_methods(core_probs, specialist_probs, expansion_probs, y_holdout):\n",
    "    \"\"\"Evaluate different ensemble methods\"\"\"\n",
    "    # Test different ensemble methods\n",
    "    ensemble_methods = {\n",
    "        'Logical OR': lambda p1, p2, p3: np.logical_or.reduce([p1 >= 0.5, p2 >= 0.5, p3 >= 0.5]),\n",
    "        'Average': lambda p1, p2, p3: (p1 + p2 + p3) / 3,\n",
    "        'Max': lambda p1, p2, p3: np.maximum.reduce([p1, p2, p3]),\n",
    "        'Weighted Average': lambda p1, p2, p3: 0.4*p1 + 0.4*p2 + 0.2*p3\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for method_name, ensemble_func in ensemble_methods.items():\n",
    "        if method_name == 'Logical OR':\n",
    "            ensemble_preds = ensemble_func(core_probs, specialist_probs, expansion_probs)\n",
    "        else:\n",
    "            ensemble_probs = ensemble_func(core_probs, specialist_probs, expansion_probs)\n",
    "            ensemble_preds = (ensemble_probs >= 0.5).astype(int)\n",
    "        \n",
    "        f1 = f1_score(y_holdout, ensemble_preds)\n",
    "        results[method_name] = f1\n",
    "        print(f\"  {method_name}: F1 = {f1:.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Ensemble evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a1c79",
   "metadata": {},
   "source": [
    "## 9. Cross-Validation for Robustness\n",
    "\n",
    "Implement proper cross-validation with feature alignment to ensure robust model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0a759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_ensemble_fixed(X_data, y_data, n_splits=3):\n",
    "    \"\"\"Fixed CV that properly handles feature alignment\"\"\"\n",
    "    print(f\"Running {n_splits}-fold cross-validation...\")\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_data, y_data)):\n",
    "        print(f\"  Fold {fold + 1}/{n_splits}...\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val = X_data.iloc[train_idx], X_data.iloc[val_idx]\n",
    "        y_train, y_val = y_data.iloc[train_idx], y_data.iloc[val_idx]\n",
    "        \n",
    "        try:\n",
    "            # Use the same feature selection approach as the main training\n",
    "            feature_cols = [col for col in X_train.columns if col not in ['label', 'protein_id', 'embedding']]\n",
    "            X_clean = X_train[feature_cols].fillna(0)\n",
    "            \n",
    "            # Select top 200 features (same as core model)\n",
    "            selector = SelectKBest(score_func=f_classif, k=200)\n",
    "            X_train_selected = selector.fit_transform(X_clean, y_train)\n",
    "            selected_features = X_clean.columns[selector.get_support()].tolist()\n",
    "            \n",
    "            # Prepare validation data with same features\n",
    "            X_val_selected = X_val[selected_features].fillna(0)\n",
    "            \n",
    "            # Train simple model\n",
    "            model = xgb.XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=42,\n",
    "                scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "            )\n",
    "            model.fit(X_train_selected, y_train)\n",
    "            \n",
    "            # Predict\n",
    "            preds = model.predict(X_val_selected)\n",
    "            cv_score = f1_score(y_val, preds)\n",
    "            cv_scores.append(cv_score)\n",
    "            \n",
    "            print(f\"    Fold {fold + 1} F1: {cv_score:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Fold {fold + 1} failed: {str(e)}\")\n",
    "            cv_scores.append(0.0)\n",
    "    \n",
    "    if cv_scores:\n",
    "        mean_cv = np.mean(cv_scores)\n",
    "        std_cv = np.std(cv_scores)\n",
    "        print(f\"  Cross-validation F1: {mean_cv:.3f} ± {std_cv:.3f}\")\n",
    "        return mean_cv, std_cv\n",
    "    else:\n",
    "        print(\"  Cross-validation failed\")\n",
    "        return 0.0, 0.0\n",
    "\n",
    "print(\"Cross-validation function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b71ce0f",
   "metadata": {},
   "source": [
    "## 10. Complete Ensemble Pipeline\n",
    "\n",
    "This section demonstrates the complete ensemble training pipeline with all components integrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dfa684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_ensemble_pipeline(train_df, holdout_df, orig_pos_df, lowconf_df):\n",
    "    \"\"\"Complete ensemble training pipeline\"\"\"\n",
    "    print(\"Starting complete ensemble pipeline...\")\n",
    "    \n",
    "    # Step 1: DBSCAN clustering\n",
    "    core_indices, specialist_indices = run_dbscan_clustering(train_df, orig_pos_df)\n",
    "    \n",
    "    # Step 2: Split training data\n",
    "    core_train, specialist_train, expansion_train = split_training_data(\n",
    "        train_df, core_indices, specialist_indices, lowconf_df\n",
    "    )\n",
    "    \n",
    "    # Step 3: Check for data leakage\n",
    "    no_leakage = check_data_leakage(holdout_df, core_train, specialist_train, expansion_train)\n",
    "    if not no_leakage:\n",
    "        print(\"⚠️  WARNING: Data leakage detected! Stopping pipeline.\")\n",
    "        return None\n",
    "    \n",
    "    # Step 4: Feature selection for each model\n",
    "    print(\"\\n5. Enhanced feature selection with improved methods...\")\n",
    "    core_optimal_k, core_score = find_optimal_features_enhanced(core_train, core_train['label'], \"Core\")\n",
    "    specialist_optimal_k, specialist_score = find_optimal_features_enhanced(specialist_train, specialist_train['label'], \"Specialist\")\n",
    "    expansion_optimal_k, expansion_score = find_optimal_features_enhanced(expansion_train, expansion_train['label'], \"Expansion\")\n",
    "    \n",
    "    print(f\"\\nOptimal feature numbers:\")\n",
    "    print(f\"  Core model: {core_optimal_k} features\")\n",
    "    print(f\"  Specialist model: {specialist_optimal_k} features\")\n",
    "    print(f\"  Expansion model: {expansion_optimal_k} features\")\n",
    "    \n",
    "    # Step 5: Get optimal features\n",
    "    print(\"\\n6. Getting optimal features for each model...\")\n",
    "    core_features = get_optimal_features_enhanced(core_train, core_optimal_k, \"Core\")\n",
    "    specialist_features = get_optimal_features_enhanced(specialist_train, specialist_optimal_k, \"Specialist\")\n",
    "    expansion_features = get_optimal_features_enhanced(expansion_train, expansion_optimal_k, \"Expansion\")\n",
    "    \n",
    "    # Step 6: Train models\n",
    "    print(\"\\n7. Training optimized models with class weights...\")\n",
    "    core_model, core_final_features = train_model_with_features_enhanced(core_train, core_features, \"Core\")\n",
    "    specialist_model, specialist_final_features = train_model_with_features_enhanced(specialist_train, specialist_features, \"Specialist\")\n",
    "    expansion_model, expansion_final_features = train_model_with_features_enhanced(expansion_train, expansion_features, \"Expansion\")\n",
    "    \n",
    "    # Step 7: Prepare holdout data\n",
    "    print(\"\\n8. Enhanced ensemble methods with improved weight optimization...\")\n",
    "    holdout_core = prepare_holdout_features_enhanced(holdout_df, core_final_features, \"Core\")\n",
    "    holdout_specialist = prepare_holdout_features_enhanced(holdout_df, specialist_final_features, \"Specialist\")\n",
    "    holdout_expansion = prepare_holdout_features_enhanced(holdout_df, expansion_final_features, \"Expansion\")\n",
    "    \n",
    "    # Step 8: Get predictions\n",
    "    core_probs = core_model.predict_proba(holdout_core)[:, 1]\n",
    "    specialist_probs = specialist_model.predict_proba(holdout_specialist)[:, 1]\n",
    "    expansion_probs = expansion_model.predict_proba(holdout_expansion)[:, 1]\n",
    "    y_holdout = holdout_df['label']\n",
    "    \n",
    "    # Step 9: Evaluate ensemble methods\n",
    "    results = evaluate_ensemble_methods(core_probs, specialist_probs, expansion_probs, y_holdout)\n",
    "    \n",
    "    # Step 10: Optimize weights\n",
    "    print(\"\\n  Optimizing ensemble weights (improved method)...\")\n",
    "    optimal_weights, optimal_f1 = optimize_weights_improved(core_probs, specialist_probs, expansion_probs, y_holdout)\n",
    "    print(f\"  Optimized weights: Core={optimal_weights[0]:.3f}, Specialist={optimal_weights[1]:.3f}, Expansion={optimal_weights[2]:.3f}\")\n",
    "    print(f\"  Optimized ensemble F1: {optimal_f1:.3f}\")\n",
    "    \n",
    "    # Add optimized ensemble to results\n",
    "    results['Optimized Weights'] = optimal_f1\n",
    "    \n",
    "    # Find best ensemble method\n",
    "    best_method = max(results, key=results.get)\n",
    "    best_f1 = results[best_method]\n",
    "    \n",
    "    print(f\"\\nBest ensemble method: {best_method} (F1 = {best_f1:.3f})\")\n",
    "    \n",
    "    # Step 11: Cross-validation\n",
    "    print(\"\\n9. Fixed cross-validation for robustness...\")\n",
    "    cv_mean, cv_std = cross_validate_ensemble_fixed(train_df, train_df['label'], n_splits=3)\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'core_model': core_model,\n",
    "        'specialist_model': specialist_model,\n",
    "        'expansion_model': expansion_model,\n",
    "        'core_features': core_final_features,\n",
    "        'specialist_features': specialist_final_features,\n",
    "        'expansion_features': expansion_final_features,\n",
    "        'optimal_weights': optimal_weights,\n",
    "        'best_method': best_method,\n",
    "        'best_f1': best_f1,\n",
    "        'cv_mean': cv_mean,\n",
    "        'cv_std': cv_std,\n",
    "        'results': results\n",
    "    }\n",
    "\n",
    "print(\"Complete ensemble pipeline function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b44ac5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "\n",
    "The pipeline produces three specialized models (Core, Specialist, Expansion) that work together to predict protein interactions with high accuracy while maintaining scientific integrity."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
